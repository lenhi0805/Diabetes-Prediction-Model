# Compute its test error rate.
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score


model = LogisticRegression(max_iter=1000)
fullmodel = model.fit(trainX, trainY)
# Define the cross-validation procedure
cv = KFold(n_splits=10, shuffle=True, random_state=1)

# Evaluate the model using cross-validation
scores = cross_val_score(model, trainX, trainY, scoring='accuracy', cv=cv, n_jobs=-1)

# Compute the estimated test error rate
print(1 - np.mean(scores))

#b) Use best-subset selection based on AIC to find the best logistic regression model

import statsmodels.api as sm
import itertools
import pandas as pd
import numpy as np
import statsmodels.api as sm
import itertools

# Load data and split into training and testing set

# Define function to fit all possible models with a given number of predictors and return the best model
def get_best_model(predictors):
    X = trainX
    X = sm.add_constant(X)
    y = trainY
    model = sm.Logit(y, X).fit()
    return model

# Perform best-subset selection based on AIC
predictor_names = list(trainX.columns)
best_model = None
best_aic = np.inf
for k in range(1, len(predictor_names) + 1):
    for combo in itertools.combinations(predictor_names, k):
        model = get_best_model(list(combo))
        if model.aic < best_aic:
            best_model = model
            best_aic = model.aic


print('Best model:', best_model.summary())

model = LogisticRegression(max_iter=1000)
logreg_subset = model.fit(trainX, trainY)

# Test error rate by 10-fold cross-validation
cv = KFold(n_splits=10, shuffle=True, random_state=1)

scores = cross_val_score(model, trainX, trainY, scoring='accuracy', cv=cv, n_jobs=-1)

# Compute the estimated test error rate
print(1 - np.mean(scores))

#c Use forward stepwise selection based on AIC to find the best logistic regression model

# Define function to fit logistic regression model using a given set of predictors and return AIC
def get_model_aic(predictors):
    X = trainX
    X = sm.add_constant(X)
    y = trainY
    model = sm.Logit(y, X).fit()
    return model.aic

# Perform forward stepwise selection based on AIC
predictor_names = list(trainX.columns)
best_model = []
current_model = []
while True:
    remaining_predictors = list(set(predictor_names) - set(current_model))
    best_aic = np.inf
    for predictor in remaining_predictors:
        candidate_predictors = current_model + [predictor]
        aic = get_model_aic(candidate_predictors)
        if aic < best_aic:
            best_model = candidate_predictors
            best_aic = aic
    if best_aic < get_model_aic(current_model):
        current_model = best_model
    else:
        break

#Fit the model 
print('Best model predictors:', best_model)
trainX2 = df2.iloc[:,3]
trainX2 = sm.add_constant(trainX2) 
model = LogisticRegression(max_iter=1000)
logreg_forward = model.fit(trainX2, trainY)
# Test errors via 10 fold cross validation 
cv = KFold(n_splits=10, shuffle=True, random_state=1)
model = LogisticRegression()
scores = cross_val_score(model, trainX2, trainY, scoring='accuracy', cv=cv, n_jobs=-1)

# Compute the estimated test error rate
print(1 - np.mean(scores))

from scipy.special import modfresnelp
#d Use backward stepwise selection based on AIC to find the best logistic regression model.
# Define function to fit logistic regression model using a given set of predictors and return AIC
def get_model_aic(predictors):
    X = trainX
    X = sm.add_constant(X)
    y = trainY
    model = sm.Logit(y, X).fit()
    return model.aic

# Perform backward stepwise selection based on AIC
predictor_names = list(trainX.columns)
current_model = predictor_names
while True:
    best_aic = get_model_aic(current_model)
    for predictor in current_model:
        candidate_predictors = list(set(current_model) - set([predictor]))
        aic = get_model_aic(candidate_predictors)
        if aic < best_aic:
            best_model = candidate_predictors
            best_aic = aic
    if best_aic < get_model_aic(current_model):
        current_model = best_model
    else:
        break


print('Best model predictors:', current_model)


# Test errors via 10 fold cross validation 
cv = KFold(n_splits=10, shuffle=True, random_state=1)
model = LogisticRegression(max_iter=1000)
logreg_backward = model.fit(trainX, trainY)
scores = cross_val_score(model, trainX, trainY, scoring='accuracy', cv=cv, n_jobs=-1)
print(1 - np.mean(scores))

#e  Use ridge regression with penalty parameter chosen optimally via 10-fold cross-validation to fit
# a logistic regression model
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
# Define parameter grid for Ridge penalty parameter alpha
param_grid = {'logreg__C': [0.01, 0.1, 1, 10, 100]}

# Define pipeline for Ridge logistic regression with standardization
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('logreg', LogisticRegression(penalty='l2', solver='saga', max_iter=1000))
])

#  10-fold cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='accuracy')
grid_search.fit(trainX, trainY)

# Fit Ridge logistic regression model using optimal value of alpha on training set
best_alpha = grid_search.best_params_['logreg__C']
logreg_ridge = LogisticRegression(penalty='l2', solver='saga', max_iter=10000, C=1/best_alpha)
logreg_ridge.fit(trainX, trainY)
# Calculate test error rate
cv_scores = cross_val_score(logreg_ridge, trainX, trainY, cv=10)
test_error_rate = 1 - cv_scores.mean()


print(test_error_rate)
print('Optimal value of alpha:', best_alpha)

#f ) Use lasso with penalty parameter chosen optimally via 10-fold cross-validation to fit a logistic
#regression model
# Define parameter grid for Lasso penalty parameter alpha
param_grid = {'logreg__C': [0.01, 0.1, 1, 10, 100]}

# Define pipeline for Lasso logistic regression with standardization
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('logreg', LogisticRegression(penalty='l1', solver='saga', max_iter=1000))
])

# Find optimal value of alpha via 10-fold cross-validation
grid_search = GridSearchCV(pipeline, param_grid, cv=10, scoring='accuracy')
grid_search.fit(trainX, trainY)

# Fit Lasso logistic regression model using optimal value of alpha on training set
best_C = grid_search.best_params_['logreg__C']
logreg_lasso = LogisticRegression(penalty='l1', solver='saga', max_iter=10000, C=best_C)
logreg_lasso.fit(trainX, trainY)

# Use cross-validation to estimate test error rate
cv_scores = cross_val_score(logreg_lasso,trainX, trainY, cv=10)
test_error_rate = 1 - cv_scores.mean()

print(best_C)
print('Estimated test error rate:', test_error_rate)

#Make a tabular summary of the parameter estimates and test error rates from (a) - (f).
# Fit different logistic regression models
from tabulate import tabulate
model_names = ['Full model', 'Best subset', 'Forward stepwise', 'Backward stepwise', 'Lasso']
models = [fullmodel, logreg_subset, logreg_forward, logreg_backward,logreg_ridge, logreg_lasso]

# Extract coefficients and estimated test error rates
coefs = [m.coef_[0] for m in models]
test_error_rates = [0.2215,0.2215,0.3419,0.2215,0.257,0.25050000000000006]


max_len = max(len(coefs), len(test_error_rates))
coefs += [np.nan] * (max_len - len(coefs))
test_error_rates += [np.nan] * (max_len - len(test_error_rates))

df = pd.DataFrame({'Model': ['fullmodel', 'logreg_subset', 'logreg_forward', 'logreg_backward','logreg_ridge', 'logreg_lasso'][:max_len],
                   'Coefficients': coefs,
                   'Test Error Rate': test_error_rates})

print(df.to_string(index=False))

