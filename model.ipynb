#2 Fit a logistic regression model using all predictors in the data.
#a
model = LogisticRegression(random_state=42, max_iter=1000).fit(trainX, trainY)
y_pred_train = model.predict(trainX)
tn, fp, fn, tp = confusion_matrix(trainY, y_pred_train).ravel()
# Calculate sensitivity and specificity
sensitivity = tp / (tp + fn)
specificity = tn / (tn + fp)

# Print results
print("Error rate:", error_rate)
print("Sensitivity:", sensitivity)
print("Specificity:", specificity)

#b Estimate the test error rate of the model in (a) using LOOCV.
from sklearn.metrics import accuracy_score

y_pred_probs = np.zeros(len(trainY))

for i in range(len(trainX)):
    # Create training data by excluding the i-th value
    X_train = trainX.drop(trainX.index[i])
    y_train = trainY.drop(trainY.index[i])
    
    # Fit the model 
    model.fit(X_train, y_train)
    
    # Predict 
    X_test = trainX.iloc[[i]]
    y_pred_probs[i] = model.predict_proba(X_test)[:, 1]
    # Store the predicted values
    
    
# test error rate
y_pred = (y_pred_probs >= 0.5).astype(int)
test_error_rate = 1 - accuracy_score(trainY, y_pred)
print('test error rate:',test_error_rate )
print(round(test_error_rate,2))

from sklearn.model_selection import LeaveOneOut, cross_val_score
loo = LeaveOneOut()
y_pred_probs = np.zeros(len(trainY))
for train_idx, test_idx in loo.split(trainX):
    # Split data into training and testing sets
    X_train, X_test = trainX.iloc[train_idx], trainX.iloc[test_idx]
    y_train, y_test = trainY[train_idx], trainY[test_idx]
    
    # Fit logistic regression model on training set

    model.fit(X_train, y_train)
    
    # Predict probability of left-out observation
    y_pred_probs[test_idx] = model.predict_proba(X_test)[:, 1]
y_pred = (y_pred_probs >= 0.5).astype(int)
test_error_rate2 = 1 - accuracy_score(trainY, y_pred)


print(f"The estimated test error rate using LOOCV is: {test_error_rate2:.3f}")

#d  Estimate the test error rate using LOOCV in the model 1
model_q1 = LogisticRegression(random_state=42, max_iter=1000).fit(trainX2, trainY)
y_pred_probs2 = np.zeros(len(trainY))
for train_idx, test_idx in loo.split(trainX):
    # Split data into training and testing sets
    X_train, X_test = trainX.iloc[train_idx], trainX.iloc[test_idx]
    y_train, y_test = trainY[train_idx], trainY[test_idx]
    
    # Fit logistic regression model on training set

    model_q1.fit(X_train, y_train)
    
    # Predict probability of left-out observation
    y_pred_probs2[test_idx] = model.predict_proba(X_test)[:, 1]
y_pred = (y_pred_probs >= 0.5).astype(int)
test_error_rate2 = 1 - accuracy_score(trainY, y_pred)


print(f"The estimated test error rate using LOOCV is: {test_error_rate2:.3f}")

#e Repeat (d) using LDA 
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
model2 = LinearDiscriminantAnalysis()
y_pred2 = []
for train_idx, test_idx in loo.split(trainX):
    # Split data into training and testing sets
    X_train, X_test = trainX.iloc[train_idx], trainX.iloc[test_idx]
    y_train, y_test = trainY[train_idx], trainY[test_idx]

    # Fit LDA model 
   
    model2.fit(X_train, y_train)

    # Predict class label of left-out observation
    y_pred2.append(model.predict(X_test)[0])

# Compute test error rate
test_error_rate2 = 1 - accuracy_score(trainY, y_pred2)

print(f"Test error rate using LOOCV with LDA: {test_error_rate2:.3f}")


#f Repeat (d) using QDA 
from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis
model3 = QuadraticDiscriminantAnalysis()
y_pred3 = []
for train_idx, test_idx in loo.split(trainX):
    # Split data into training and testing sets
    X_train, X_test = trainX.iloc[train_idx], trainX.iloc[test_idx]
    y_train, y_test = trainY[train_idx], trainY[test_idx]

    # Fit LDA model 
   
    model3.fit(X_train, y_train)

    # Predict class label of left-out observation
    y_pred3.append(model.predict(X_test)[0])

# Compute test error rate
test_error_rate3 = 1 - accuracy_score(trainY, y_pred3)

print(f"Test error rate using LOOCV with QDA: {test_error_rate3:.3f}")

# g the test error rate for knn model using optimal K value
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV
param_grid = {'n_neighbors': list(range(1, 20))}

knn = KNeighborsClassifier()

#find the optimal value of k
optimalK = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')
optimalK.fit(trainX, trainY)
model4 = KNeighborsClassifier(n_neighbors=1)
## Estimate the test error rate using LOOCV.

y_pred4 = []
for train_idx, test_idx in loo.split(trainX):
    # Split data into training and testing sets
    X_train, X_test = trainX.iloc[train_idx], trainX.iloc[test_idx]
    y_train, y_test = trainY[train_idx], trainY[test_idx]

    # Fit LDA model 
   
    model4.fit(X_train, y_train)

    # Predict class label of left-out observation
    y_pred4.append(model.predict(X_test)[0])

# Compute test error rate
test_error_rate4 = 1 - accuracy_score(trainY, y_pred4)

print(f"Test error rate using LOOCV with KNN: {test_error_rate4:.3f}")
